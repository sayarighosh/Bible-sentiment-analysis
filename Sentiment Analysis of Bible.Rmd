---
title: "Sentiment Analysis of Bible"
author: "Sayari"
output: html_document
---

Sentiment Analysis and topic modelling for Bible. 

Libraries used 
```{r}
library(tm)
library(plyr)
library(httr)
library(stringr)
library(tm)
library(tibble)
#The rest of these libraries are used for visualization
library(ggplot2)
library(wordcloud)
library(cluster)
library(topicmodels)
library(SnowballC)
```

Reading the Bible data
```{r}
bible_full = readLines("C:\\Users\\Sayari Ghosh\\Desktop\\santa clara university\\2016\\Spring2017\\R\\ProgramCode\\data_files\\ascii_bible.txt")
```

Data Cleaning
```{r}
#remove leading whitespaces
bible_text = str_replace(bible_full,"^\\s+","")  
res = grep("[0-9][0-9][0-9]:[0-9][0-9][0-9]",bible_text)
temp = NULL
for (i in seq(1:(length(res)-1))){
  x = paste(bible_text[res[i]:(res[i+1]-1)],collapse = " ")
  temp = c(temp,x)
}
verses = c(temp,paste(bible_text[res[length(res)]:length(bible_text)],collapse = " "))
verses[1:5]
```

Chapters length 
```{r}
chap = grep("[0-9][0-9][0-9]:001",verses)
length(chap)
```

Taking only the string part and removing the chapter and verse numbers from the 
Bible text. 

```{r}
ver_txt =  str_split_fixed(verses, "[0-9][0-9][0-9]:[0-9][0-9][0-9] ",2)
ver_txt = ver_txt[,2]
```

Using the TM package in R to cleaning and process the data. This includes 
converting to lower case, remove numbers, remove punctation, remove common 
stopwords, strip whitespace, and get rid of special characters. I also utilized 
stemming, which should cut words to their base. 

Converting the text to lower case.
```{r}
ver_txt = str_to_lower(ver_txt)
ver_txt[1:5]
```

Convert the text of all verses into a Corpus.

```{r}
ver_txt = Corpus(VectorSource(ver_txt))
ver_txt
```

Remove all punctuation  
```{r}
ver_txt = tm_map(ver_txt,removePunctuation)
```

Remove all stopwords.  
```{r}
ver_stp = ver_txt
ver_stp = tm_map(ver_stp,removeWords,stopwords("english"))
```

Now stem the text, to remove multiplicity of similar words of the same root. 
```{r}
ver_stem = tm_map(ver_stp,stemDocument)
ver_uniq = DocumentTermMatrix(ver_stem)
ver_uniq
```

How many distinct words are there in the bible, after stemming?
```{r}
ver_tdm = DocumentTermMatrix(ver_stem)
ver_tdm
```

The 5 most common words in the bible. 
```{r}
ver_mat = as.matrix(ver_tdm)
cm_words = sort(colSums(ver_mat), decreasing = TRUE) #frequency for each term
w_5 = head(cm_words,5)
w_5 #5 most common words
```

Top 10 frequent word in Bible
```{r}
w_10 = head(cm_words,10)
w_10 = rownames_to_column(data.frame(w_10), var="word")  
ggplot(w_10, aes(x=word,y=w_10)) + geom_bar(stat="identity") + coord_flip()
```

5 least common words
```{r}
cm_words1 = sort(colSums(ver_mat), decreasing = FALSE) #frequency for each term
u_5 = head(cm_words1,5)
u_5 
```

This is the result of clustering of the text in Bible. I have taken top 70 words and created 5 cluster from it using hierarchical clustering method. 
```{r}
h_70 = head(cm_words,70)
h_70 = as.matrix(h_70)
#Creates the distance matrix
distMatrix = dist(scale(h_70))
fit = hclust(distMatrix)
plot(fit, cex=0.67)
rect.hclust(fit, k=5) #5 clusters
```

wordcloud of the top 100 words in the bible. 
```{r}
pal2 <- brewer.pal(8,"Dark2")
w_100 = head(cm_words,100) 
w_100_nms = names(w_100) #names of top 100 words
wordcloud(w_100_nms,w_100,scale=c(5, .1),min.freq=2,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2) #creating word cloud


```

Mood score the original text of the bible (before stemming)
```{r}
#creating positive and negitive words
HIDict = readLines("C:\\Users\\Sayari Ghosh\\Desktop\\santa clara university\\2016\\Spring2017\\R\\ProgramCode\\data_files\\inqdict.txt")
dict_pos = HIDict[grep("Pos",HIDict)]
poswords = NULL
for (s in dict_pos) {
    s = strsplit(s,"#")[[1]][1]
    poswords = c(poswords,strsplit(s," ")[[1]][1])
}
dict_neg = HIDict[grep("Neg",HIDict)]
negwords = NULL
for (s in dict_neg) {
    s = strsplit(s,"#")[[1]][1]
    negwords = c(negwords,strsplit(s," ")[[1]][1])
}
poswords = tolower(poswords)
negwords = tolower(negwords)
poswords = unique(poswords)
negwords = unique(negwords)

#Mood Scoring the bible text
ver_txt =  str_split_fixed(verses, "[0-9][0-9][0-9]:[0-9][0-9][0-9] ",2)
ver_txt = ver_txt[,2] #bible text
ver_txt = Corpus(VectorSource(ver_txt))
ver_txt = tm_map(ver_txt,removePunctuation)
v1 = data.frame(text = sapply(ver_txt, as.character), stringsAsFactors = FALSE) #corpus to dataframe
v1 = str_replace(v1$text,"^\\s+","")  #remove leading whitespaces
v1 = trimws(v1,which = "right") # triming trailing whitespaces
v1 = str_to_lower(v1) 
ver_txt1 = unlist(strsplit(v1," ")) #spliting by space to get words
posmatch = match(ver_txt1,poswords)
numposmatch = length(posmatch[which(posmatch>0)])
negmatch = match(ver_txt1,negwords)
numnegmatch = length(negmatch[which(negmatch>0)])
print(c(numposmatch,numnegmatch))
```
The Bible is more positive words than negative words so it can be concluded that 
the overall sentiment is positive. 


Topic Modelling
The main 3 topics in the bible, and the top 25 words in each topic. 
```{r}
burnin = 4000
iter = 2000
thin = 500
seed = list(2003,5,63,100001,765)
nstart = 5
best = TRUE

#Number of topics
k = 3

res <-LDA(ver_tdm, k, method="Gibbs", control = list(nstart = nstart, seed = seed, best = best, burnin = burnin, iter = iter, thin = thin))

#Show topics
res.topics = as.matrix(topics(res))
#print(res.topics)
res.terms = as.matrix(terms(res,25))
print(res.terms)
```

